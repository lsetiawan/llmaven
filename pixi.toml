"$schema" = "https://pixi.sh/v0.55.0/schema/manifest/schema.json"

[workspace]
name = "LLMaven"
version = "0.1.0"
description = "An AI Powered Tool library for Scientific Discovery"
authors = ["UW SSEC Team"]
requires-pixi = ">=0.55.0"
channels = ["conda-forge", "pytorch"]
platforms = ["linux-64", "osx-arm64"]

[tasks]
start-jlab = { cmd = "jupyter lab", description = "Start Jupyter Lab" }
serve-panel = { cmd = "python legacy/rubin-panel-app.py", description = "Serve the panel app" }

[dependencies]
python = "3.11.*"
pixi-kernel = ">=0.4.0,<0.5"
jupyterlab = ">=4.2.5,<4.3"
jupyter_bokeh = ">=4.0.5,<4.1"
libopenblas = ">=0.3.27,<0.4"
tqdm = ">=4.66.5,<4.67"
numpy = "<2"
pandas = ">=2.2.2,<2.3"
panel = ">=1.5.0,<1.6"
ipykernel = ">=6.29.5,<6.30"
docker-py = ">=7.1.0,<7.2"
testcontainers = ">=4.8.0,<4.9"
tenacity = "==8.5.0"
pytest = ">=8.3.5,<9"

[pypi-dependencies]
langchain = "~=0.3.26"
langchain-community = "~=0.3.26"
langchain-qdrant = ">=0.1.0"
langchain-huggingface = ">=0.0.3"
jupyter-panel-proxy = "==0.2.0a2"
qdrant-client = ">=1.11.2, <1.12"
sentence-transformers = ">=3.1.0, <3.2"
nltk = ">=3.9.1, <3.10"
arxiv = ">=2.1.3, <2.2"
pymupdf = ">=1.24.10, <1.25"
ssec-tutorials = { git = "https://github.com/uw-ssec/ssec_tutorials.git", rev = "98ca4cf84c06be6a4de99d3a4a1dff5a442c95c7" }
transformers = "~=4.53.0"
accelerate = ">=0.26.0"
bitsandbytes = ">=0.42.0, <0.46"
llama_cpp_python = "~=0.3.9"
openai = ">=2.3"


[feature.infra.dependencies]
python = "3.11.*"
azure-data-tables = ">=12.7.0,<13"
python-dotenv = ">=1.1.1,<2"
typer = ">=0.20.0,<0.21"
rich = ">=14.2.0,<15"

[feature.infra.pypi-dependencies]
pulumi = ">=3.203.0, <4"
pulumi-azure-native = ">=3.8.0, <4"

[feature.proxy.dependencies]
python = "3.11.*"
adlfs = ">=2025.8.0, <2026"
aiohttp = ">=3.13.0,<4"
propcache = ">=0.2.0"
fastapi = ">=0.119.1,<0.120"
uvicorn = ">=0.38.0,<0.39"
httpx = ">=0.28.1,<0.29"
python-dotenv = ">=1.1.1,<2"
azure-data-tables = ">=12.7.0,<13"

[target.linux-64.pypi-dependencies]
llama-cpp-python = { url = "https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl" }

[feature.frontend.dependencies]
streamlit = "*"
tenacity = "==8.5.0"


[environments]
frontend = { features = ["frontend"], no-default-feature = true }
infra = { features = [
    "infra",
], solve-group = "infra", no-default-feature = true }
proxy = { features = [
    "proxy",
], solve-group = "proxy", no-default-feature = true }
